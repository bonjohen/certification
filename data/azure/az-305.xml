<?xml version='1.0' encoding='UTF-8'?>
<certification-exam xmlns="http://certification.study/schema/v1" version="1.0">
  <metadata>
    <exam-code>AZ-305</exam-code>
    <exam-title>Designing Microsoft Azure Infrastructure Solutions</exam-title>
    <provider>Microsoft</provider>
    <description>Scenario-Based Study Companion for AZ-305 certification - validates expertise in designing cloud and hybrid solutions for identity, governance, data storage, business continuity, and infrastructure.</description>
    <total-questions>50</total-questions>
    <created-date>2026-01-20</created-date>
    <last-modified>2026-01-20T00:00:00Z</last-modified>
    <categories>
      <category id="cat-identity">Identity, Governance, and Monitoring</category>
      <category id="cat-data">Data Storage Solutions</category>
      <category id="cat-continuity">Business Continuity</category>
      <category id="cat-infrastructure">Infrastructure Solutions</category>
    </categories>
  </metadata>

  <questions>
    <question id="1" category-ref="cat-identity" difficulty="advanced">
      <title>Hybrid Identity Architecture</title>
      <scenario>A multinational company with 50,000 employees is migrating to Azure. They have on-premises Active Directory with complex GPOs, require seamless SSO to both cloud and on-premises apps, and must maintain existing security policies during migration.</scenario>
      <question-text>Which identity architecture should you recommend?</question-text>
      <choices>
        <choice letter="A">Federation with AD FS for all authentication</choice>
        <choice letter="B">Cloud-only Entra ID with manual user provisioning</choice>
        <choice letter="C">Azure AD Connect with Pass-through Authentication only</choice>
        <choice letter="D">Azure AD Connect with Password Hash Sync and Seamless SSO</choice>
      </choices>
      <correct-answer>D</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Password Hash Sync with Seamless SSO provides the best balance of simplicity, resilience, and user experience.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure AD Connect with Password Hash Sync (PHS) and Seamless SSO is the recommended approach. PHS provides resilience (works even if on-premises is down), enables leaked credential detection, and Seamless SSO provides frictionless sign-in from domain-joined devices without AD FS complexity.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>PHS is Microsoft's recommended default; it syncs password hashes (not actual passwords) to Entra ID.</li>
              <li>Pass-through Authentication (PTA) validates against on-premises AD but adds dependency.</li>
              <li>AD FS adds complexity and infrastructure; use only when federation is required (third-party MFA, smart cards).</li>
              <li>PHS enables Identity Protection risk-based policies and leaked credential reports.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Hybrid Identity</tag>
        <tag>Azure AD Connect</tag>
        <tag>Password Hash Sync</tag>
      </tags>
    </question>

    <question id="2" category-ref="cat-identity" difficulty="advanced">
      <title>Multi-tenant SaaS Identity</title>
      <scenario>You're designing a multi-tenant SaaS application. Each customer (tenant) wants their users to sign in with their own corporate Entra ID. Some customers require Conditional Access policies to apply.</scenario>
      <question-text>Which identity pattern should you implement?</question-text>
      <choices>
        <choice letter="A">Azure AD B2B with external identities and cross-tenant access policies</choice>
        <choice letter="B">Single Entra ID tenant with all customer users as members</choice>
        <choice letter="C">Azure AD B2C with local accounts for all users</choice>
        <choice letter="D">Separate Entra ID tenant per customer managed by you</choice>
      </choices>
      <correct-answer>A</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>B2B collaboration allows external users to sign in with their home tenant credentials while your app controls access.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure AD B2B allows customers to authenticate with their corporate Entra ID (home tenant) while your application manages authorization. Cross-tenant access policies let you control collaboration settings. Customer Conditional Access policies apply during authentication.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>B2B: Business-to-business, external users from other organizations; B2C: Consumer identities.</li>
              <li>Cross-tenant access settings control inbound/outbound collaboration per organization.</li>
              <li>B2B direct connect enables seamless Teams shared channels without guest accounts.</li>
              <li>Multi-tenant app registration allows sign-in from any Entra ID tenant.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>B2B</tag>
        <tag>Multi-tenant</tag>
        <tag>SaaS Architecture</tag>
      </tags>
    </question>

    <question id="3" category-ref="cat-identity" difficulty="advanced">
      <title>Privileged Access Management</title>
      <scenario>Your organization needs to reduce standing administrative access. Admins should request elevated permissions only when needed, with approval workflows and time-limited access.</scenario>
      <question-text>Which solution should you implement?</question-text>
      <choices>
        <choice letter="A">Permanent role assignments with MFA</choice>
        <choice letter="B">Azure AD Privileged Identity Management (PIM)</choice>
        <choice letter="C">Conditional Access policies only</choice>
        <choice letter="D">Custom approval workflow with Logic Apps</choice>
      </choices>
      <correct-answer>B</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>PIM provides just-in-time privileged access with activation workflows and audit trails.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure AD Privileged Identity Management (PIM) enables just-in-time (JIT) access to privileged roles. Users are eligible for roles but must activate them, optionally requiring approval, MFA, and justification. Access is time-limited and fully audited.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>PIM covers both Entra ID roles and Azure resource roles (RBAC).</li>
              <li>Access reviews ensure ongoing validation of role assignments.</li>
              <li>Alerts notify on suspicious activation patterns.</li>
              <li>Requires Entra ID P2 license.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>PIM</tag>
        <tag>Privileged Access</tag>
        <tag>Zero Trust</tag>
      </tags>
    </question>

    <question id="4" category-ref="cat-identity" difficulty="advanced">
      <title>Governance at Scale</title>
      <scenario>An enterprise with 200 subscriptions across multiple business units needs centralized governance. They require consistent tagging, allowed regions, and security baselines applied automatically to new subscriptions.</scenario>
      <question-text>What architecture should you design?</question-text>
      <choices>
        <choice letter="A">Apply policies individually to each subscription</choice>
        <choice letter="B">Management group hierarchy with Azure Policy initiatives and Blueprints</choice>
        <choice letter="C">Single subscription with resource groups per business unit</choice>
        <choice letter="D">ARM templates with governance resources in each deployment</choice>
      </choices>
      <correct-answer>B</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Management groups enable hierarchical policy inheritance across subscriptions.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Design a management group hierarchy (e.g., Root &gt; Platform/Landing Zones &gt; Prod/NonProd &gt; BU). Assign Azure Policy initiatives at appropriate scopes for automatic inheritance. Use Blueprints to deploy standard resources and policies to new subscriptions (landing zone pattern).</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Azure Landing Zone architecture provides proven management group structure.</li>
              <li>Policy initiatives bundle related policies; built-in initiatives exist for common standards (CIS, NIST).</li>
              <li>DeployIfNotExists policies can automatically remediate non-compliance.</li>
              <li>Exemptions should be time-limited and tracked.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Management Groups</tag>
        <tag>Azure Policy</tag>
        <tag>Landing Zones</tag>
      </tags>
    </question>

    <question id="5" category-ref="cat-identity" difficulty="advanced">
      <title>Monitoring Strategy</title>
      <scenario>You need a unified monitoring solution for 500 VMs across multiple subscriptions, with centralized alerting, dashboards, and log retention for compliance (7 years for audit logs).</scenario>
      <question-text>What monitoring architecture should you design?</question-text>
      <choices>
        <choice letter="A">Azure Monitor only without Log Analytics</choice>
        <choice letter="B">Separate Log Analytics workspace per subscription</choice>
        <choice letter="C">Centralized Log Analytics workspace with Azure Monitor, archive to storage for long-term retention</choice>
        <choice letter="D">Third-party monitoring solution exclusively</choice>
      </choices>
      <correct-answer>C</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Centralized Log Analytics enables unified querying; archive to storage for cost-effective long-term retention.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Use a centralized Log Analytics workspace for operational data (up to 2 years interactive retention). Configure diagnostic settings to send logs from all resources. For 7-year compliance retention, export to Azure Storage (archive tier) or use Log Analytics data export.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Workspace design: centralized for small/medium; federated for large enterprises with data sovereignty needs.</li>
              <li>Azure Monitor Agent (AMA) replaces legacy agents; supports Data Collection Rules.</li>
              <li>Archive tier storage costs ~$0.002/GB/month for long-term retention.</li>
              <li>Log Analytics workspace-based Application Insights recommended for new deployments.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Azure Monitor</tag>
        <tag>Log Analytics</tag>
        <tag>Compliance</tag>
      </tags>
    </question>

    <question id="6" category-ref="cat-data" difficulty="advanced">
      <title>Relational Data Platform Selection</title>
      <scenario>A retail company needs to migrate a 10TB SQL Server database with 500+ stored procedures, CLR assemblies, and SQL Agent jobs. They want minimal code changes and full SQL Server feature compatibility.</scenario>
      <question-text>Which Azure data platform should you recommend?</question-text>
      <choices>
        <choice letter="A">SQL Server on Azure VM</choice>
        <choice letter="B">Azure SQL Database (single database)</choice>
        <choice letter="C">Azure SQL Managed Instance</choice>
        <choice letter="D">Azure Database for PostgreSQL</choice>
      </choices>
      <correct-answer>C</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>SQL Managed Instance provides near 100% SQL Server compatibility with PaaS benefits.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure SQL Managed Instance offers near 100% compatibility with on-premises SQL Server, including SQL Agent, CLR, cross-database queries, and linked servers. It's a PaaS service so you don't manage OS or patching, unlike SQL on VM.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>SQL MI supports up to 16TB storage; for larger, use SQL on VM or partition data.</li>
              <li>Azure SQL Database lacks some features: CLR, SQL Agent, cross-database queries.</li>
              <li>SQL on VM gives full control but requires you to manage HA, backups, and patching.</li>
              <li>Use Azure Database Migration Service for online migrations with minimal downtime.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>SQL Managed Instance</tag>
        <tag>Database Migration</tag>
        <tag>PaaS</tag>
      </tags>
    </question>

    <question id="7" category-ref="cat-data" difficulty="advanced">
      <title>NoSQL Database Selection</title>
      <scenario>A gaming company needs a database for player profiles and game state with single-digit millisecond reads globally. Players are distributed worldwide and need consistent experience regardless of location.</scenario>
      <question-text>Which database solution should you recommend?</question-text>
      <choices>
        <choice letter="A">Azure SQL Database with geo-replication</choice>
        <choice letter="B">Azure Cosmos DB with multi-region writes</choice>
        <choice letter="C">Azure Cache for Redis</choice>
        <choice letter="D">Azure Table Storage</choice>
      </choices>
      <correct-answer>B</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Cosmos DB provides guaranteed single-digit millisecond latency with turnkey global distribution.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure Cosmos DB offers single-digit millisecond response times at any scale with 99.999% availability when using multi-region configuration. Multi-region writes allow players to write to their nearest region, eliminating write latency caused by geo-distance.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Choose appropriate consistency level: Session consistency often balances performance and correctness for gaming.</li>
              <li>Partition key design is critical: choose a key with high cardinality and even distribution.</li>
              <li>Multi-region writes require conflict resolution policy (Last Writer Wins or custom).</li>
              <li>Consider serverless for variable/unpredictable workloads; provisioned throughput for steady-state.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Cosmos DB</tag>
        <tag>Global Distribution</tag>
        <tag>NoSQL</tag>
      </tags>
    </question>

    <question id="8" category-ref="cat-data" difficulty="advanced">
      <title>Data Lake Architecture</title>
      <scenario>An analytics team needs to store petabytes of raw data (logs, IoT telemetry, documents) for big data processing with Spark and machine learning workloads.</scenario>
      <question-text>Which storage solution should you design?</question-text>
      <choices>
        <choice letter="A">Azure Blob Storage without hierarchical namespace</choice>
        <choice letter="B">Azure Data Lake Storage Gen2 with hierarchical namespace</choice>
        <choice letter="C">Azure Files Premium</choice>
        <choice letter="D">Azure SQL Data Warehouse</choice>
      </choices>
      <correct-answer>B</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>ADLS Gen2 combines Blob Storage scale with hierarchical file system for analytics.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure Data Lake Storage Gen2 is built on Blob Storage with hierarchical namespace enabled. This provides file system semantics (directories, atomic operations) required for big data frameworks like Spark, Hadoop, and Databricks while maintaining Blob Storage's massive scale and cost efficiency.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Hierarchical namespace enables efficient directory operations (rename, delete).</li>
              <li>ABFS driver optimized for analytics workloads.</li>
              <li>Use ACLs for fine-grained access control in addition to RBAC.</li>
              <li>Organize data in zones: raw, enriched, curated (medallion architecture).</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Data Lake</tag>
        <tag>ADLS Gen2</tag>
        <tag>Big Data</tag>
      </tags>
    </question>

    <question id="9" category-ref="cat-data" difficulty="advanced">
      <title>Storage Account Security</title>
      <scenario>A healthcare application stores PHI (Protected Health Information) in Blob Storage. Requirements: data must be encrypted with customer-managed keys, accessible only from specific VNets, and all access must be audited.</scenario>
      <question-text>Which combination of features should you configure?</question-text>
      <choices>
        <choice letter="A">Infrastructure encryption only, firewall rules, Activity Log only</choice>
        <choice letter="B">Microsoft-managed keys, Service Endpoints, no logging</choice>
        <choice letter="C">Customer-managed keys, public access with SAS tokens, diagnostic logging</choice>
        <choice letter="D">Customer-managed keys in Key Vault, Private Endpoints, Storage Analytics logging</choice>
      </choices>
      <correct-answer>D</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>CMK provides key control, Private Endpoints eliminate public exposure, logging enables audit.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Use customer-managed keys (CMK) in Azure Key Vault for encryption key control. Private Endpoints provide private IP access from your VNet without internet exposure. Enable Storage Analytics logging or Azure Monitor diagnostic settings to audit all read/write operations.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>CMK allows key rotation and revocation; consider double encryption (infrastructure + CMK).</li>
              <li>Private Endpoints vs Service Endpoints: PE provides private IP; SE routes over Microsoft backbone but uses public IP.</li>
              <li>Disable public blob access at the storage account level for sensitive data.</li>
              <li>Enable soft delete and versioning for data protection.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Storage Security</tag>
        <tag>Private Endpoints</tag>
        <tag>Encryption</tag>
      </tags>
    </question>

    <question id="10" category-ref="cat-data" difficulty="advanced">
      <title>Caching Strategy</title>
      <scenario>An e-commerce site experiences high database load from repeated queries for product catalog data. Product data changes infrequently but reads are extremely high during sales events.</scenario>
      <question-text>What caching architecture should you implement?</question-text>
      <choices>
        <choice letter="A">Azure CDN for all API responses</choice>
        <choice letter="B">Azure Cache for Redis with cache-aside pattern and TTL-based expiration</choice>
        <choice letter="C">In-memory caching on each app instance only</choice>
        <choice letter="D">Database query result caching only</choice>
      </choices>
      <correct-answer>B</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Redis provides distributed caching with sub-millisecond latency; cache-aside pattern gives control over cache updates.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure Cache for Redis provides a distributed, in-memory cache. Use the cache-aside pattern: check cache first, on miss read from database and populate cache with TTL. This reduces database load while ensuring data freshness through expiration.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Cache-aside: application manages cache; write-through: writes go to cache and DB together.</li>
              <li>Consider cache warming before sales events to prevent thundering herd.</li>
              <li>Use Redis data structures (sorted sets, hashes) for complex scenarios.</li>
              <li>Premium tier provides clustering, geo-replication, and persistence.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Redis Cache</tag>
        <tag>Caching Patterns</tag>
        <tag>Performance</tag>
      </tags>
    </question>

    <question id="11" category-ref="cat-data" difficulty="advanced">
      <title>Message-based Integration</title>
      <scenario>An order processing system needs to decouple order placement from fulfillment. Orders must never be lost, processing must be exactly-once, and the system must handle 10,000 orders/second during peak.</scenario>
      <question-text>Which messaging solution should you design?</question-text>
      <choices>
        <choice letter="A">Azure Event Grid</choice>
        <choice letter="B">Azure Queue Storage</choice>
        <choice letter="C">Azure Service Bus Premium with sessions and duplicate detection</choice>
        <choice letter="D">Azure Event Hubs</choice>
      </choices>
      <correct-answer>C</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Service Bus Premium provides guaranteed delivery, exactly-once processing with sessions, and high throughput.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure Service Bus Premium tier provides high throughput (up to millions of messages/day), guaranteed delivery, and enterprise features. Sessions enable ordered processing and exactly-once semantics. Duplicate detection prevents reprocessing if producers retry.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Service Bus: transactional messaging; Event Hubs: high-throughput event streaming.</li>
              <li>Sessions group related messages and provide FIFO within a session.</li>
              <li>Dead-letter queue handles poison messages.</li>
              <li>Premium tier provides dedicated resources, 1MB messages, and geo-DR.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Service Bus</tag>
        <tag>Messaging</tag>
        <tag>Decoupling</tag>
      </tags>
    </question>

    <question id="12" category-ref="cat-data" difficulty="advanced">
      <title>Event Streaming Architecture</title>
      <scenario>An IoT platform ingests telemetry from 1 million devices sending data every second. Data needs real-time processing for anomaly detection and long-term storage for analytics.</scenario>
      <question-text>What architecture should you design?</question-text>
      <choices>
        <choice letter="A">Azure IoT Hub only with direct database writes</choice>
        <choice letter="B">Azure Service Bus with Azure Functions</choice>
        <choice letter="C">Azure Event Hubs with Azure Stream Analytics and ADLS Gen2</choice>
        <choice letter="D">Azure Queue Storage with batch processing</choice>
      </choices>
      <correct-answer>C</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Event Hubs handles massive ingestion; Stream Analytics provides real-time processing; ADLS stores for analytics.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Event Hubs can ingest millions of events/second with low latency. Azure Stream Analytics processes streams in real-time using SQL-like queries for anomaly detection. Event Hubs Capture or Stream Analytics outputs to ADLS Gen2 for long-term storage and batch analytics.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Event Hubs partitions enable parallel processing; partition key distributes events.</li>
              <li>Consider IoT Hub for device management features (twins, direct methods, provisioning).</li>
              <li>Event Hubs Capture automatically writes to storage in Avro format.</li>
              <li>Kafka protocol support allows existing Kafka applications to use Event Hubs.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Event Hubs</tag>
        <tag>Stream Analytics</tag>
        <tag>IoT</tag>
      </tags>
    </question>

    <question id="13" category-ref="cat-continuity" difficulty="advanced">
      <title>Multi-Region DR Strategy</title>
      <scenario>A financial services application requires RPO of 5 minutes and RTO of 1 hour. The application uses Azure SQL Database and App Service. Cost optimization is important but must not compromise recovery objectives.</scenario>
      <question-text>Which disaster recovery architecture should you design?</question-text>
      <choices>
        <choice letter="A">Single region with availability zones only</choice>
        <choice letter="B">Active-active with Traffic Manager and synchronized databases</choice>
        <choice letter="C">Backup and restore only</choice>
        <choice letter="D">Active-passive with Azure SQL geo-replication and standby App Service Plan</choice>
      </choices>
      <correct-answer>D</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Active-passive balances cost and RTO; geo-replication provides near-zero RPO for database.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure SQL active geo-replication continuously replicates to a secondary region (RPO ~5 seconds). Deploy App Service in the secondary region at minimum scale (cost optimization), scale up during failover. Use Azure Traffic Manager or Front Door for automatic failover detection.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Geo-replication RPO depends on network latency; typically under 5 seconds.</li>
              <li>Auto-failover groups automate failover for SQL Database.</li>
              <li>App Service deployment slots can pre-warm the standby environment.</li>
              <li>Test failover regularly; document runbooks for DR procedures.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Disaster Recovery</tag>
        <tag>Geo-replication</tag>
        <tag>RPO/RTO</tag>
      </tags>
    </question>

    <question id="14" category-ref="cat-continuity" difficulty="advanced">
      <title>Backup Strategy Design</title>
      <scenario>An organization needs comprehensive backup for Azure VMs, SQL databases, file shares, and blob data. Backups must be retained for 7 years for compliance, with some data requiring immutable storage.</scenario>
      <question-text>What backup architecture should you implement?</question-text>
      <choices>
        <choice letter="A">Third-party backup solution exclusively</choice>
        <choice letter="B">VM snapshots stored in the same region only</choice>
        <choice letter="C">Azure Backup with Recovery Services Vault, long-term retention policies, and immutable vaults</choice>
        <choice letter="D">Application-level backups to blob storage without centralized management</choice>
      </choices>
      <correct-answer>C</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Azure Backup provides unified backup management with long-term retention and immutability options.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure Backup with Recovery Services Vault provides centralized backup for VMs, SQL, Files, and blobs. Configure backup policies with long-term retention (up to 99 years). Enable immutable vaults to protect backups from deletion or modification (ransomware protection).</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Backup Center provides single-pane management across vaults and subscriptions.</li>
              <li>GRS vault replication protects backups from regional disaster.</li>
              <li>Immutable vaults prevent backup deletion even by administrators.</li>
              <li>MARS agent backs up on-premises files; DPM/MABS for complex on-premises workloads.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Azure Backup</tag>
        <tag>Recovery Services</tag>
        <tag>Compliance</tag>
      </tags>
    </question>

    <question id="15" category-ref="cat-continuity" difficulty="advanced">
      <title>High Availability for VMs</title>
      <scenario>A mission-critical SAP HANA database requires 99.99% availability. The VM must survive datacenter-level failures within a region and have predictable low-latency storage.</scenario>
      <question-text>Which VM deployment configuration should you design?</question-text>
      <choices>
        <choice letter="A">Virtual Machine Scale Set in single zone</choice>
        <choice letter="B">Single VM with Premium SSD</choice>
        <choice letter="C">Availability Set with Standard HDD</choice>
        <choice letter="D">Availability Zones with Ultra Disks or Premium SSD v2</choice>
      </choices>
      <correct-answer>D</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Availability Zones provide 99.99% SLA; Ultra Disks provide predictable sub-millisecond latency required for HANA.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Deploy VMs across Availability Zones for 99.99% SLA and datacenter-level fault tolerance. Use Ultra Disks or Premium SSD v2 for SAP HANA's demanding storage requirements (consistent sub-millisecond latency, high IOPS). Zone-redundant disks replicate across zones.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Availability Zones: 99.99% SLA; Availability Sets: 99.95% SLA; Single VM with Premium: 99.9%.</li>
              <li>Ultra Disks: up to 160,000 IOPS and 4,000 MBps per disk.</li>
              <li>Zone-redundant storage (ZRS) for managed disks provides zone redundancy.</li>
              <li>SAP HANA certified M-series VMs for production workloads.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Availability Zones</tag>
        <tag>Ultra Disks</tag>
        <tag>SAP HANA</tag>
      </tags>
    </question>

    <question id="16" category-ref="cat-continuity" difficulty="advanced">
      <title>Azure Site Recovery Design</title>
      <scenario>A company needs to replicate on-premises VMware VMs to Azure for disaster recovery. They want automated failover with minimal RTO and the ability to test DR without impacting production.</scenario>
      <question-text>How should you design the replication?</question-text>
      <choices>
        <choice letter="A">Manual VM exports and imports during disaster</choice>
        <choice letter="B">Azure Site Recovery with recovery plans and isolated test failover network</choice>
        <choice letter="C">Azure Migrate for one-time migration only</choice>
        <choice letter="D">Backup and restore from Recovery Services Vault</choice>
      </choices>
      <correct-answer>B</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>ASR provides continuous replication, orchestrated failover, and non-disruptive DR testing.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure Site Recovery (ASR) continuously replicates VMware VMs to Azure. Recovery plans orchestrate failover sequence with scripts and manual actions. Test failover creates VMs in an isolated network without affecting replication or production, enabling regular DR drills.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>ASR supports VMware, Hyper-V, physical servers, and Azure-to-Azure replication.</li>
              <li>Recovery plans group VMs and define failover order with custom scripts.</li>
              <li>Application-consistent snapshots ensure database integrity.</li>
              <li>Replication frequency: continuous (Hyper-V/VMware) or every 5/15/30 minutes.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Site Recovery</tag>
        <tag>VMware</tag>
        <tag>Disaster Recovery</tag>
      </tags>
    </question>

    <question id="17" category-ref="cat-infrastructure" difficulty="advanced">
      <title>Hub-Spoke Network Topology</title>
      <scenario>An enterprise needs to connect 20 spoke VNets to shared services (firewall, DNS, AD) while maintaining network isolation between business units. Traffic between spokes must be inspected.</scenario>
      <question-text>Which network architecture should you design?</question-text>
      <choices>
        <choice letter="A">Separate VNets with VPN connections between them</choice>
        <choice letter="B">Full mesh VNet peering between all VNets</choice>
        <choice letter="C">Single large VNet with subnets for each business unit</choice>
        <choice letter="D">Hub-spoke with Azure Firewall in hub and UDRs routing spoke-to-spoke traffic through hub</choice>
      </choices>
      <correct-answer>D</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Hub-spoke centralizes shared services; UDRs force traffic through firewall for inspection.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Hub-spoke topology places shared services (Azure Firewall, DNS, NVAs) in a central hub VNet. Spokes peer to hub only, providing isolation. User-defined routes (UDRs) on spoke subnets route all traffic through Azure Firewall for inspection and logging.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Enable "Allow Gateway Transit" on hub and "Use Remote Gateway" on spokes for on-premises connectivity.</li>
              <li>Azure Virtual WAN simplifies large hub-spoke deployments with managed hubs.</li>
              <li>Azure Firewall Manager provides centralized policy management.</li>
              <li>Consider Azure Route Server for dynamic routing with NVAs.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Hub-Spoke</tag>
        <tag>Network Design</tag>
        <tag>Azure Firewall</tag>
      </tags>
    </question>

    <question id="18" category-ref="cat-infrastructure" difficulty="advanced">
      <title>Hybrid Connectivity Design</title>
      <scenario>A company requires dedicated private connectivity from their datacenter to Azure with 10 Gbps bandwidth, redundancy, and the ability to connect to multiple Azure regions.</scenario>
      <question-text>Which connectivity solution should you design?</question-text>
      <choices>
        <choice letter="A">ExpressRoute with Global Reach and redundant circuits</choice>
        <choice letter="B">Site-to-site VPN only</choice>
        <choice letter="C">Point-to-site VPN for all users</choice>
        <choice letter="D">Azure Peering Service</choice>
      </choices>
      <correct-answer>A</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>ExpressRoute provides dedicated private connectivity up to 100 Gbps; Global Reach connects circuits across regions.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>ExpressRoute provides dedicated private connectivity with predictable performance. Deploy redundant circuits (different providers/locations) for high availability. ExpressRoute Global Reach allows traffic to flow between on-premises sites through Microsoft's network, connecting multiple regions.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>ExpressRoute circuits: 50 Mbps to 100 Gbps; Direct provides 10/100 Gbps port.</li>
              <li>Premium add-on enables global routing and more VNet connections.</li>
              <li>ExpressRoute is not encrypted by default; layer VPN for encryption if needed.</li>
              <li>Consider ExpressRoute + VPN failover for maximum resiliency.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>ExpressRoute</tag>
        <tag>Hybrid Connectivity</tag>
        <tag>Global Reach</tag>
      </tags>
    </question>

    <question id="19" category-ref="cat-infrastructure" difficulty="advanced">
      <title>Web Application Architecture</title>
      <scenario>A global web application requires sub-100ms response times for users worldwide, SSL offloading, WAF protection, and intelligent routing to the nearest healthy backend.</scenario>
      <question-text>Which Azure services should you combine?</question-text>
      <choices>
        <choice letter="A">Azure CDN only</choice>
        <choice letter="B">Azure Load Balancer only</choice>
        <choice letter="C">Azure Traffic Manager with Application Gateway per region</choice>
        <choice letter="D">Azure Front Door with WAF policy and backend pools in multiple regions</choice>
      </choices>
      <correct-answer>D</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Front Door provides global load balancing, WAF, SSL termination, and anycast for low latency.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure Front Door is a global Layer 7 load balancer with anycast (users connect to nearest edge). It provides SSL offloading, integrated WAF, intelligent routing based on latency/priority/weighted, and health probes. Backend pools can span Azure regions and even external endpoints.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Front Door vs Traffic Manager: Front Door is Layer 7 with data path; Traffic Manager is DNS-based.</li>
              <li>Front Door Standard/Premium tiers include CDN integration.</li>
              <li>WAF policies protect against OWASP top 10 and bot attacks.</li>
              <li>Private Link origins allow Front Door to connect to private backends.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Front Door</tag>
        <tag>WAF</tag>
        <tag>Global Load Balancing</tag>
      </tags>
    </question>

    <question id="20" category-ref="cat-infrastructure" difficulty="advanced">
      <title>Container Platform Selection</title>
      <scenario>A team is containerizing 50 microservices. They need service discovery, automatic scaling, rolling deployments, secrets management, and integration with Azure services. The team has Kubernetes experience.</scenario>
      <question-text>Which container platform should you recommend?</question-text>
      <choices>
        <choice letter="A">Azure App Service for Containers</choice>
        <choice letter="B">Azure Container Instances for all services</choice>
        <choice letter="C">Azure Kubernetes Service (AKS) with managed add-ons</choice>
        <choice letter="D">Self-managed Kubernetes on VMs</choice>
      </choices>
      <correct-answer>C</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>AKS provides managed Kubernetes with Azure integration; ideal for microservices at scale.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>AKS is the managed Kubernetes service with Azure handling the control plane. Managed add-ons provide service mesh (Istio), monitoring (Azure Monitor), secrets (Key Vault CSI driver), and GitOps (Flux). The team's Kubernetes experience transfers directly.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>AKS control plane is free; pay only for worker nodes.</li>
              <li>KEDA provides event-driven autoscaling beyond HPA.</li>
              <li>Azure CNI vs Kubenet: CNI gives pods VNet IPs; Kubenet uses NAT.</li>
              <li>AKS workload identity replaces pod identity for Azure AD integration.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>AKS</tag>
        <tag>Kubernetes</tag>
        <tag>Microservices</tag>
      </tags>
    </question>

    <question id="21" category-ref="cat-infrastructure" difficulty="advanced">
      <title>Serverless Application Design</title>
      <scenario>An application processes uploaded documents: extract text, analyze sentiment, store results. Processing should scale to zero when idle and handle burst uploads. Costs must be minimized.</scenario>
      <question-text>Which architecture should you design?</question-text>
      <choices>
        <choice letter="A">App Service with scheduled jobs</choice>
        <choice letter="B">Always-running VMs polling for new files</choice>
        <choice letter="C">Azure Batch for all processing</choice>
        <choice letter="D">Azure Functions (Consumption) triggered by Blob Storage, using Cognitive Services, output to Cosmos DB</choice>
      </choices>
      <correct-answer>D</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Functions Consumption plan scales automatically and charges only for execution time.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure Functions on Consumption plan scales automatically based on events (blob trigger). It scales to zero when idle, minimizing costs. Integrate with Cognitive Services for text extraction (Form Recognizer) and sentiment analysis (Language). Store structured results in Cosmos DB.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Consumption plan: pay per execution; 5-minute default timeout (10 max).</li>
              <li>Use Durable Functions for long-running or stateful workflows.</li>
              <li>Consider Premium plan for VNet integration and longer execution times.</li>
              <li>Blob trigger uses Storage queues internally; consider Event Grid trigger for lower latency.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Azure Functions</tag>
        <tag>Serverless</tag>
        <tag>Event-driven</tag>
      </tags>
    </question>

    <question id="22" category-ref="cat-infrastructure" difficulty="advanced">
      <title>API Management Design</title>
      <scenario>A company exposes APIs to partners and internal developers. Requirements: rate limiting, authentication via OAuth, versioning, developer portal, and analytics on API usage.</scenario>
      <question-text>Which service should be the centerpiece of the API architecture?</question-text>
      <choices>
        <choice letter="A">Azure Front Door only</choice>
        <choice letter="B">Azure API Management with policies and developer portal</choice>
        <choice letter="C">Custom API gateway built on App Service</choice>
        <choice letter="D">Azure Functions with HTTP triggers only</choice>
      </choices>
      <correct-answer>B</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>APIM provides full API lifecycle management with policies, portal, and analytics.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure API Management (APIM) provides API gateway, developer portal, and management plane. Policies handle rate limiting, JWT validation, transformation, and caching. Built-in analytics show usage patterns. Products and subscriptions control access levels.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>APIM tiers: Consumption (serverless), Developer, Basic, Standard, Premium.</li>
              <li>Premium provides multi-region deployment and VNet integration.</li>
              <li>Self-hosted gateway extends APIM to on-premises or other clouds.</li>
              <li>Versioning strategies: path, query string, or header-based.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>API Management</tag>
        <tag>API Gateway</tag>
        <tag>Developer Portal</tag>
      </tags>
    </question>

    <question id="23" category-ref="cat-infrastructure" difficulty="advanced">
      <title>Compute Cost Optimization</title>
      <scenario>A batch processing workload runs nightly for 4 hours, processing large datasets. The workload is fault-tolerant and can be interrupted. Cost is the primary concern.</scenario>
      <question-text>Which compute strategy should you recommend?</question-text>
      <choices>
        <choice letter="A">Reserved Instances for 3 years</choice>
        <choice letter="B">Azure Spot Virtual Machines with checkpointing</choice>
        <choice letter="C">Pay-as-you-go VMs running 24/7</choice>
        <choice letter="D">Dedicated Hosts</choice>
      </choices>
      <correct-answer>B</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Spot VMs provide up to 90% discount for interruptible, fault-tolerant workloads.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure Spot VMs use spare Azure capacity at up to 90% discount. Since the workload is fault-tolerant and runs only 4 hours, Spot is ideal. Implement checkpointing to save progress and resume if evicted. Reserved Instances suit steady-state workloads, not intermittent ones.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Spot VMs can be evicted with 30 seconds notice when Azure needs capacity.</li>
              <li>Set max price to control costs; use -1 for market price.</li>
              <li>Eviction policy: Stop-Deallocate (resume later) or Delete.</li>
              <li>Azure Batch natively supports Spot VMs with automatic retry.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Spot VMs</tag>
        <tag>Cost Optimization</tag>
        <tag>Batch Processing</tag>
      </tags>
    </question>

    <question id="24" category-ref="cat-infrastructure" difficulty="advanced">
      <title>VM Scale Set Design</title>
      <scenario>A web tier needs to handle variable traffic, automatically scaling between 2-20 instances. Instances must be distributed across fault domains, and deployments should not cause downtime.</scenario>
      <question-text>How should you configure the scale set?</question-text>
      <choices>
        <choice letter="A">VMSS with Flexible orchestration, autoscale rules, and rolling upgrade policy</choice>
        <choice letter="B">Manual scaling with availability set</choice>
        <choice letter="C">VMSS Uniform with manual instance management</choice>
        <choice letter="D">Individual VMs with custom scaling scripts</choice>
      </choices>
      <correct-answer>A</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Flexible VMSS provides fault domain spreading, autoscale, and rolling upgrades for zero-downtime updates.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>VM Scale Sets with Flexible orchestration spread instances across fault domains automatically. Configure autoscale rules based on metrics (CPU, memory, queue depth) with min/max instance counts. Rolling upgrade policy updates instances in batches, maintaining availability during deployments.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Flexible vs Uniform: Flexible offers more VM features and mixing; Uniform is simpler.</li>
              <li>Overprovisioning reduces scale-out time but may incur brief extra costs.</li>
              <li>Health probes with automatic repair replace unhealthy instances.</li>
              <li>Instance protection prevents specific instances from scale-in/updates.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>VMSS</tag>
        <tag>Autoscaling</tag>
        <tag>High Availability</tag>
      </tags>
    </question>

    <question id="25" category-ref="cat-identity" difficulty="advanced">
      <title>Workload Identity</title>
      <scenario>Applications running in AKS need to access Azure Key Vault and Storage without storing credentials in code or configuration.</scenario>
      <question-text>Which identity solution should you implement?</question-text>
      <choices>
        <choice letter="A">Azure AD Workload Identity with federated credentials</choice>
        <choice letter="B">Store service principal credentials in Kubernetes secrets</choice>
        <choice letter="C">Managed Identity for VM only</choice>
        <choice letter="D">Shared access signatures hardcoded in application</choice>
      </choices>
      <correct-answer>A</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Workload Identity enables pods to authenticate to Azure AD without managing credentials.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure AD Workload Identity federates Kubernetes service accounts with Azure AD. Pods use projected service account tokens to authenticate, eliminating credential management. Grant the workload identity Azure RBAC roles for Key Vault and Storage access.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Workload Identity replaces the deprecated AAD Pod Identity.</li>
              <li>Uses OIDC federation; no secrets stored in cluster.</li>
              <li>Create federated credential linking Kubernetes SA to Azure AD app registration.</li>
              <li>Works with any workload supporting OIDC token exchange.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Workload Identity</tag>
        <tag>AKS</tag>
        <tag>Credential-free</tag>
      </tags>
    </question>

    <question id="26" category-ref="cat-data" difficulty="advanced">
      <title>Data Integration Architecture</title>
      <scenario>A data platform needs to ingest data from 50+ sources (databases, SaaS apps, files), transform it, and load into a data warehouse. Pipelines should be visual, manageable, and cost-effective.</scenario>
      <question-text>Which service should orchestrate the data integration?</question-text>
      <choices>
        <choice letter="A">Azure Data Factory with integration runtimes and mapping data flows</choice>
        <choice letter="B">Custom ETL code on VMs</choice>
        <choice letter="C">Azure Logic Apps only</choice>
        <choice letter="D">SQL Server Integration Services on Azure VM</choice>
      </choices>
      <correct-answer>A</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Data Factory provides managed ETL/ELT with 100+ connectors and visual authoring.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure Data Factory (ADF) is a managed data integration service with 100+ built-in connectors. Mapping data flows provide visual transformation logic executed on Spark. Integration runtimes connect to on-premises and cloud sources. Pay only for pipeline execution time.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Self-hosted Integration Runtime enables on-premises connectivity.</li>
              <li>Data flows execute on Spark clusters; enable debug for development.</li>
              <li>Use triggers for scheduling, event-based, or tumbling window patterns.</li>
              <li>Synapse Pipelines provides similar capabilities integrated with Synapse Analytics.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Data Factory</tag>
        <tag>ETL</tag>
        <tag>Data Integration</tag>
      </tags>
    </question>

    <question id="27" category-ref="cat-data" difficulty="advanced">
      <title>Analytics Platform Design</title>
      <scenario>A company needs a unified analytics platform for data warehousing, big data processing, and machine learning. Data scientists want to use Spark notebooks; analysts want SQL access.</scenario>
      <question-text>Which platform should you design around?</question-text>
      <choices>
        <choice letter="A">Azure Synapse Analytics with dedicated SQL pool, serverless SQL, and Spark pools</choice>
        <choice letter="B">Separate Azure SQL Database and HDInsight clusters</choice>
        <choice letter="C">Azure Databricks only</choice>
        <choice letter="D">SQL Server on Azure VM with SSIS</choice>
      </choices>
      <correct-answer>A</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Synapse provides unified experience for SQL, Spark, and data integration in one workspace.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure Synapse Analytics integrates dedicated SQL pools (data warehouse), serverless SQL (query data lake directly), and Spark pools (big data/ML) in a unified workspace. Analysts use SQL; data scientists use Spark notebooks. Synapse Pipelines handles orchestration.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Dedicated SQL pool: provisioned MPP warehouse; serverless: pay-per-query.</li>
              <li>Synapse Link provides near-real-time analytics on operational data (Cosmos DB, SQL).</li>
              <li>Workspace managed VNet provides network isolation.</li>
              <li>Consider Databricks for advanced ML and Delta Lake capabilities.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Synapse Analytics</tag>
        <tag>Data Warehouse</tag>
        <tag>Big Data</tag>
      </tags>
    </question>

    <question id="28" category-ref="cat-identity" difficulty="advanced">
      <title>Zero Trust Network Access</title>
      <scenario>Remote workers need secure access to internal web applications without VPN. Access should verify user identity, device compliance, and apply least-privilege per application.</scenario>
      <question-text>Which solution enables zero trust application access?</question-text>
      <choices>
        <choice letter="A">Expose applications directly to internet with firewall</choice>
        <choice letter="B">Site-to-site VPN for all users</choice>
        <choice letter="C">Azure AD Application Proxy with Conditional Access</choice>
        <choice letter="D">Remote Desktop Gateway</choice>
      </choices>
      <correct-answer>C</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>App Proxy publishes internal apps externally with Azure AD authentication and Conditional Access.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure AD Application Proxy publishes on-premises web applications without inbound firewall rules or VPN. Users authenticate through Azure AD, enabling Conditional Access policies (MFA, compliant device, location). Each application can have individual access policies.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>App Proxy Connector runs on-premises; outbound-only communication to Azure.</li>
              <li>Pre-authentication options: Azure AD (recommended) or passthrough.</li>
              <li>Supports header-based, Kerberos, and password-based SSO to backend apps.</li>
              <li>Consider Microsoft Entra Private Access for broader zero trust network access.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Application Proxy</tag>
        <tag>Zero Trust</tag>
        <tag>Remote Access</tag>
      </tags>
    </question>

    <question id="29" category-ref="cat-infrastructure" difficulty="advanced">
      <title>Landing Zone Architecture</title>
      <scenario>An enterprise is adopting Azure at scale. They need a standardized foundation with networking, identity, governance, and security that supports autonomous workload teams.</scenario>
      <question-text>What should be your starting point for the architecture?</question-text>
      <choices>
        <choice letter="A">Manual setup of each component independently</choice>
        <choice letter="B">Single subscription for all workloads</choice>
        <choice letter="C">Azure Landing Zone accelerator with platform and application landing zones</choice>
        <choice letter="D">Third-party infrastructure automation only</choice>
      </choices>
      <correct-answer>C</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Azure Landing Zone provides prescriptive architecture and automation for enterprise-scale adoption.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure Landing Zone accelerator provides a conceptual architecture and reference implementations for enterprise-scale Azure adoption. It includes management group hierarchy, policies, hub networking, identity integration, and subscription vending for workload teams.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Platform landing zones: shared services (connectivity, identity, management).</li>
              <li>Application landing zones: workload subscriptions with inherited governance.</li>
              <li>Subscription vending automates new subscription provisioning with policies.</li>
              <li>Cloud Adoption Framework provides guidance; Landing Zone accelerators provide implementation.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Landing Zones</tag>
        <tag>Enterprise Scale</tag>
        <tag>Cloud Adoption</tag>
      </tags>
    </question>

    <question id="30" category-ref="cat-continuity" difficulty="advanced">
      <title>Application Resilience Patterns</title>
      <scenario>A microservices application needs to handle transient failures gracefully, prevent cascade failures, and maintain responsiveness when downstream services are slow.</scenario>
      <question-text>Which resilience patterns should the architecture implement?</question-text>
      <choices>
        <choice letter="A">No error handling; let failures propagate</choice>
        <choice letter="B">Retry with exponential backoff, circuit breaker, and timeout patterns</choice>
        <choice letter="C">Synchronous retry loops without delay</choice>
        <choice letter="D">Single point of failure with no redundancy</choice>
      </choices>
      <correct-answer>B</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Combine retry for transient failures, circuit breaker to prevent cascades, and timeouts for responsiveness.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Retry with exponential backoff handles transient failures without overwhelming services. Circuit breaker pattern stops calling failing services, allowing recovery. Timeouts prevent indefinite waiting. Together, these patterns create resilient applications that degrade gracefully.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Add jitter to retries to prevent thundering herd.</li>
              <li>Circuit breaker states: Closed (normal), Open (failing), Half-Open (testing recovery).</li>
              <li>Bulkhead pattern isolates failures to prevent resource exhaustion.</li>
              <li>Libraries: Polly (.NET), Resilience4j (Java), or service mesh (Istio/Linkerd).</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Resilience Patterns</tag>
        <tag>Circuit Breaker</tag>
        <tag>Microservices</tag>
      </tags>
    </question>

    <question id="31" category-ref="cat-infrastructure" difficulty="advanced">
      <title>Private Endpoint Design</title>
      <scenario>All PaaS services (Storage, SQL, Key Vault) must be accessible only from your VNet. No traffic should traverse the public internet.</scenario>
      <question-text>How should you design the network connectivity?</question-text>
      <choices>
        <choice letter="A">Firewall rules allowing VNet IP ranges</choice>
        <choice letter="B">Service Endpoints only</choice>
        <choice letter="C">Private Endpoints for each service with Private DNS zones</choice>
        <choice letter="D">Public endpoints with SAS authentication</choice>
      </choices>
      <correct-answer>C</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Private Endpoints create private IPs in your VNet; Private DNS resolves service names to private IPs.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Private Endpoints create a network interface with a private IP in your VNet for PaaS services. Traffic stays entirely on Microsoft's network. Private DNS zones ensure service FQDNs resolve to private IPs from your VNet. Disable public network access on services.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Private Endpoint vs Service Endpoint: PE has private IP; SE uses public IP over Microsoft backbone.</li>
              <li>Centralize Private DNS zones in hub and link to spoke VNets.</li>
              <li>Azure Private Link service exposes your own services privately.</li>
              <li>NSG on Private Endpoint subnet controls access (supported for most services).</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Private Endpoints</tag>
        <tag>Private Link</tag>
        <tag>Network Security</tag>
      </tags>
    </question>

    <question id="32" category-ref="cat-data" difficulty="advanced">
      <title>Database Partitioning Strategy</title>
      <scenario>A multi-tenant SaaS application stores tenant data in Azure SQL Database. Tenants vary greatly in size (1GB to 1TB). You need to balance cost, performance isolation, and management overhead.</scenario>
      <question-text>What partitioning strategy should you implement?</question-text>
      <choices>
        <choice letter="A">Separate SQL Server per tenant</choice>
        <choice letter="B">Single database with tenant ID column</choice>
        <choice letter="C">Elastic pools with tenant-per-database and tiered pools by size</choice>
        <choice letter="D">Cosmos DB with single partition key</choice>
      </choices>
      <correct-answer>C</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Elastic pools share resources efficiently; tenant-per-database provides isolation; tiered pools match resources to tenant size.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Use database-per-tenant for isolation and easy management (backup, restore, move). Group databases into elastic pools to share resources efficiently. Create tiered pools (small/medium/large) and place tenants based on their size. Move large tenants to dedicated databases.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Elastic pools: tenants share eDTUs/vCores; burst within pool limits.</li>
              <li>Elastic Jobs runs maintenance across pooled databases.</li>
              <li>Shard Map Manager routes connections in sharded applications.</li>
              <li>Consider Hyperscale for very large tenants (100TB+ support).</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Elastic Pools</tag>
        <tag>Multi-tenant</tag>
        <tag>Partitioning</tag>
      </tags>
    </question>

    <question id="33" category-ref="cat-infrastructure" difficulty="advanced">
      <title>DevOps Integration</title>
      <scenario>Development teams need to provision environments on-demand, deploy applications through pipelines, and enforce infrastructure standards. Infrastructure must be versioned and repeatable.</scenario>
      <question-text>What infrastructure-as-code approach should you standardize on?</question-text>
      <choices>
        <choice letter="A">ARM templates emailed between teams</choice>
        <choice letter="B">Manual portal deployments documented in wiki</choice>
        <choice letter="C">PowerShell scripts with hardcoded values</choice>
        <choice letter="D">Bicep templates in Git with CI/CD pipelines and Azure Policy guardrails</choice>
      </choices>
      <correct-answer>D</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Bicep provides readable IaC; Git enables versioning; CI/CD automates deployment; Policy enforces standards.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Bicep is a domain-specific language that compiles to ARM templates with cleaner syntax. Store templates in Git for version control and code review. CI/CD pipelines (Azure DevOps, GitHub Actions) automate deployments. Azure Policy ensures deployed resources comply with standards.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Bicep modules enable reusable, composable infrastructure components.</li>
              <li>Deployment stacks manage lifecycle of template-deployed resources.</li>
              <li>What-if operations preview changes before deployment.</li>
              <li>Consider Terraform for multi-cloud IaC requirements.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Bicep</tag>
        <tag>IaC</tag>
        <tag>DevOps</tag>
      </tags>
    </question>

    <question id="34" category-ref="cat-security" difficulty="advanced">
      <title>Security Architecture</title>
      <scenario>A defense-in-depth security architecture is required. The application handles sensitive data and must meet SOC 2 compliance with centralized security monitoring.</scenario>
      <question-text>Which combination of services provides comprehensive security?</question-text>
      <choices>
        <choice letter="A">Azure Firewall without logging</choice>
        <choice letter="B">Network Security Groups only</choice>
        <choice letter="C">Microsoft Defender for Cloud, Microsoft Sentinel, and Key Vault with managed HSM</choice>
        <choice letter="D">Third-party antivirus on VMs only</choice>
      </choices>
      <correct-answer>C</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Defender for Cloud provides posture management; Sentinel provides SIEM/SOAR; Key Vault HSM protects keys.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Microsoft Defender for Cloud provides security posture management and workload protection. Microsoft Sentinel provides cloud-native SIEM/SOAR for centralized security monitoring and automated response. Azure Key Vault with managed HSM provides FIPS 140-2 Level 3 key protection.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Defender for Cloud regulatory compliance dashboard tracks SOC 2, PCI-DSS, etc.</li>
              <li>Sentinel uses Log Analytics; connect Defender, Azure AD, and custom sources.</li>
              <li>Managed HSM provides dedicated HSM for high-security key operations.</li>
              <li>Use Azure Policy to enforce security configurations continuously.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Defender for Cloud</tag>
        <tag>Sentinel</tag>
        <tag>Security Architecture</tag>
      </tags>
    </question>

    <question id="35" category-ref="cat-continuity" difficulty="advanced">
      <title>SLA Composition</title>
      <scenario>Your application uses Azure App Service (99.95% SLA) connecting to Azure SQL Database (99.99% SLA). Both must be available for the application to work.</scenario>
      <question-text>What is the composite SLA for the application?</question-text>
      <choices>
        <choice letter="A">99.94% (multiply individual SLAs)</choice>
        <choice letter="B">99.99% (take the highest)</choice>
        <choice letter="C">99.95% (take the lowest)</choice>
        <choice letter="D">199.94% (add SLAs together)</choice>
      </choices>
      <correct-answer>A</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Composite SLA is calculated by multiplying dependent service SLAs.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>When services are in series (both required), multiply their SLAs: 99.95% x 99.99% = 99.94%. Each additional required component reduces overall SLA. To improve composite SLA, add redundancy (parallel paths) or use higher-SLA service tiers.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Parallel (redundant) paths improve SLA: 1 - (1 - SLA1) x (1 - SLA2).</li>
              <li>More components in series = lower composite SLA.</li>
              <li>Design for failure: assume components will fail and plan accordingly.</li>
              <li>Application-level resilience can compensate for infrastructure SLA gaps.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>SLA</tag>
        <tag>Composite SLA</tag>
        <tag>Reliability</tag>
      </tags>
    </question>

    <question id="36" category-ref="cat-infrastructure" difficulty="advanced">
      <title>DNS Architecture</title>
      <scenario>Your hybrid environment requires unified DNS resolution: Azure resources must resolve on-premises names, and on-premises must resolve Azure Private Endpoint names.</scenario>
      <question-text>What DNS architecture should you implement?</question-text>
      <choices>
        <choice letter="A">Hosts file entries on all machines</choice>
        <choice letter="B">Custom DNS servers on VMs with conditional forwarders</choice>
        <choice letter="C">Public DNS only</choice>
        <choice letter="D">Azure Private DNS Resolver with inbound and outbound endpoints</choice>
      </choices>
      <correct-answer>D</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Azure Private DNS Resolver provides managed DNS forwarding between Azure and on-premises.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure Private DNS Resolver provides inbound endpoints (on-premises resolves Azure private DNS) and outbound endpoints (Azure VNets resolve on-premises DNS). This eliminates the need for custom DNS VMs while enabling hybrid DNS resolution.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Inbound endpoint: on-premises forwards queries to Azure private zones.</li>
              <li>Outbound endpoint with DNS forwarding ruleset: Azure forwards to on-premises DNS.</li>
              <li>Link Private DNS zones to hub VNet; use DNS Resolver for cross-premises.</li>
              <li>Consider Azure Firewall DNS proxy for centralized DNS with FQDN filtering.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>DNS Resolver</tag>
        <tag>Hybrid DNS</tag>
        <tag>Private DNS</tag>
      </tags>
    </question>

    <question id="37" category-ref="cat-data" difficulty="advanced">
      <title>Data Encryption Strategy</title>
      <scenario>Regulations require encryption at rest with customer-controlled keys, encryption in transit, and the ability to prove data was never accessed by the cloud provider.</scenario>
      <question-text>What encryption architecture meets these requirements?</question-text>
      <choices>
        <choice letter="A">Customer-managed keys with Azure Confidential Computing for sensitive workloads</choice>
        <choice letter="B">Microsoft-managed keys only</choice>
        <choice letter="C">No encryption with network isolation</choice>
        <choice letter="D">Application-level encryption only</choice>
      </choices>
      <correct-answer>A</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>CMK provides key control; Confidential Computing protects data in use with attestation.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Customer-managed keys (CMK) in Key Vault provide control over encryption keys. For proving data isolation even from the provider, Azure Confidential Computing uses hardware-based trusted execution environments (TEEs) with remote attestation to verify code integrity.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>CMK: customer controls key lifecycle, rotation, and revocation.</li>
              <li>Double encryption: infrastructure encryption + CMK for defense in depth.</li>
              <li>Confidential VMs use AMD SEV-SNP or Intel TDX for memory encryption.</li>
              <li>Azure Attestation verifies TEE integrity before processing sensitive data.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Encryption</tag>
        <tag>Confidential Computing</tag>
        <tag>Data Protection</tag>
      </tags>
    </question>

    <question id="38" category-ref="cat-infrastructure" difficulty="advanced">
      <title>Multi-region Deployment</title>
      <scenario>A globally distributed application requires active-active deployment across regions with automatic failover. Users should always connect to the nearest healthy region.</scenario>
      <question-text>Which traffic management approach should you use?</question-text>
      <choices>
        <choice letter="A">Round-robin DNS</choice>
        <choice letter="B">Azure Front Door with priority and latency-based routing</choice>
        <choice letter="C">Azure Load Balancer across regions</choice>
        <choice letter="D">Manual DNS updates during failover</choice>
      </choices>
      <correct-answer>B</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Front Door provides global load balancing with automatic failover and latency-based routing.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure Front Door provides global Layer 7 load balancing with health probes. Latency-based routing directs users to the nearest healthy backend. Priority settings enable automatic failover to secondary regions. Anycast ensures users connect to the nearest edge location.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Front Door supports weighted, latency, priority, and session affinity routing.</li>
              <li>Health probes detect failures; failover is automatic and fast.</li>
              <li>Consider Traffic Manager for DNS-based routing if Layer 7 isn't needed.</li>
              <li>Active-active requires conflict resolution strategy for databases (e.g., Cosmos DB multi-write).</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Multi-region</tag>
        <tag>Front Door</tag>
        <tag>Global Load Balancing</tag>
      </tags>
    </question>

    <question id="39" category-ref="cat-identity" difficulty="advanced">
      <title>Service Principal Security</title>
      <scenario>CI/CD pipelines need to deploy Azure resources. You want to avoid storing secrets and enable time-limited, auditable deployments.</scenario>
      <question-text>What authentication approach should the pipelines use?</question-text>
      <choices>
        <choice letter="A">Personal access tokens of developers</choice>
        <choice letter="B">Service principal with client secret stored in pipeline variables</choice>
        <choice letter="C">Workload identity federation with OIDC from the CI/CD platform</choice>
        <choice letter="D">Shared admin credentials</choice>
      </choices>
      <correct-answer>C</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Workload identity federation eliminates secrets by using OIDC tokens from the CI/CD platform.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Workload identity federation allows CI/CD platforms (GitHub Actions, Azure DevOps) to authenticate to Azure using OIDC tokens without stored secrets. Configure a federated credential on an Azure AD application that trusts tokens from the CI/CD provider.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>GitHub Actions: configure subject claim with repo, branch, and environment.</li>
              <li>Azure DevOps: service connections support workload identity federation.</li>
              <li>Tokens are short-lived; no secrets to rotate or leak.</li>
              <li>Restrict federated credentials to specific repos/branches for security.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Workload Identity</tag>
        <tag>CI/CD</tag>
        <tag>Secretless</tag>
      </tags>
    </question>

    <question id="40" category-ref="cat-data" difficulty="advanced">
      <title>Real-time Data Architecture</title>
      <scenario>A trading platform needs sub-second data freshness from operational database to analytics dashboards. The operational database is Azure Cosmos DB.</scenario>
      <question-text>What architecture provides near-real-time analytics without impacting operational workload?</question-text>
      <choices>
        <choice letter="A">Azure Synapse Link for Cosmos DB with serverless SQL pool</choice>
        <choice letter="B">Periodic ETL jobs every hour</choice>
        <choice letter="C">Direct queries against Cosmos DB from Power BI</choice>
        <choice letter="D">Manual data exports</choice>
      </choices>
      <correct-answer>A</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Synapse Link provides automatic, near-real-time sync from Cosmos DB to analytical store.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure Synapse Link automatically replicates Cosmos DB data to a columnar analytical store without ETL. Changes sync within seconds. Serverless SQL pool or Spark can query the analytical store without impacting transactional workload. Power BI connects for dashboards.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Analytical store uses column-oriented format optimized for analytics.</li>
              <li>No RU consumption for analytical queries; separate from transactional store.</li>
              <li>Analytical TTL can differ from transactional TTL.</li>
              <li>Synapse Link also available for SQL Server and Dataverse.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Synapse Link</tag>
        <tag>Real-time Analytics</tag>
        <tag>HTAP</tag>
      </tags>
    </question>

    <question id="41" category-ref="cat-infrastructure" difficulty="advanced">
      <title>Network Security Design</title>
      <scenario>Web applications need protection against OWASP top 10, DDoS attacks, and must have centralized network policy enforcement across all workloads.</scenario>
      <question-text>What combination of network security services should you deploy?</question-text>
      <choices>
        <choice letter="A">Azure Front Door with WAF, Azure DDoS Protection Standard, and Azure Firewall</choice>
        <choice letter="B">Network Security Groups only</choice>
        <choice letter="C">Third-party firewall without Azure integration</choice>
        <choice letter="D">Application Gateway without WAF</choice>
      </choices>
      <correct-answer>A</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>WAF protects against OWASP; DDoS Protection mitigates volumetric attacks; Azure Firewall provides centralized network policy.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure Front Door (or Application Gateway) with WAF protects against OWASP top 10 and bot attacks at Layer 7. DDoS Protection Standard provides enhanced protection with attack telemetry. Azure Firewall in hub VNet provides centralized Layer 3-7 network policy enforcement.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>WAF policies: managed rules for OWASP, bot protection, custom rules.</li>
              <li>DDoS Standard: adaptive tuning, attack analytics, cost protection guarantee.</li>
              <li>Azure Firewall Premium: TLS inspection, IDPS, URL filtering.</li>
              <li>NSGs complement but don't replace firewall; NSGs are per-subnet/NIC.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>WAF</tag>
        <tag>DDoS</tag>
        <tag>Network Security</tag>
      </tags>
    </question>

    <question id="42" category-ref="cat-identity" difficulty="advanced">
      <title>Entitlement Management</title>
      <scenario>External partners need temporary access to specific Azure resources and applications. Access should be self-service, time-limited, and automatically removed when the project ends.</scenario>
      <question-text>Which solution provides governed access lifecycle?</question-text>
      <choices>
        <choice letter="A">Azure AD Entitlement Management with access packages</choice>
        <choice letter="B">Manual guest user invitations with no expiration</choice>
        <choice letter="C">Shared accounts for partners</choice>
        <choice letter="D">VPN access to everything</choice>
      </choices>
      <correct-answer>A</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Entitlement Management bundles resources into access packages with policies for request, approval, and expiration.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure AD Entitlement Management creates access packages bundling group memberships, app access, and SharePoint sites. External users request access through a portal; policies control approval workflows and automatic expiration. Access reviews ensure ongoing validity.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Connected organizations enable streamlined access for known partners.</li>
              <li>Policies define who can request, approvers, duration, and extensions.</li>
              <li>Separation of duties: requestor can't be sole approver.</li>
              <li>Requires Entra ID P2 license.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Entitlement Management</tag>
        <tag>Access Lifecycle</tag>
        <tag>B2B</tag>
      </tags>
    </question>

    <question id="43" category-ref="cat-data" difficulty="advanced">
      <title>Search Architecture</title>
      <scenario>An e-commerce platform needs full-text search with faceted navigation, typo tolerance, and relevance tuning. The catalog has 10 million products.</scenario>
      <question-text>Which service should power the search functionality?</question-text>
      <choices>
        <choice letter="A">In-memory filtering in application</choice>
        <choice letter="B">SQL Server full-text search</choice>
        <choice letter="C">Cosmos DB with LIKE queries</choice>
        <choice letter="D">Azure AI Search (Cognitive Search) with custom scoring profiles</choice>
      </choices>
      <correct-answer>D</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Azure AI Search provides full-text search, faceting, fuzzy matching, and relevance tuning.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure AI Search (formerly Cognitive Search) is a managed search-as-a-service. It provides full-text search with analyzers, faceted navigation for filtering, fuzzy matching for typo tolerance, and scoring profiles for relevance tuning. Indexers sync data from various sources.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Semantic search uses AI for natural language understanding (Premium feature).</li>
              <li>Vector search enables hybrid search with embeddings.</li>
              <li>AI enrichment pipeline extracts insights from unstructured content.</li>
              <li>Replicas provide query scale; partitions provide index size scale.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Azure AI Search</tag>
        <tag>Full-text Search</tag>
        <tag>E-commerce</tag>
      </tags>
    </question>

    <question id="44" category-ref="cat-infrastructure" difficulty="advanced">
      <title>Bastion and Secure Access</title>
      <scenario>Administrators need to access VMs in private subnets without public IPs or VPN. Access must be audited and time-limited.</scenario>
      <question-text>What solution provides secure VM access?</question-text>
      <choices>
        <choice letter="A">Jump box VM with permanent public IP</choice>
        <choice letter="B">Public IPs on all VMs with NSG rules</choice>
        <choice letter="C">Always-on VPN for all administrators</choice>
        <choice letter="D">Azure Bastion with just-in-time VM access from Defender for Cloud</choice>
      </choices>
      <correct-answer>D</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Bastion provides browser-based RDP/SSH without public IPs; JIT opens access only when needed.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure Bastion provides secure RDP/SSH access through the browser without exposing public IPs on VMs. Just-in-time (JIT) VM access from Defender for Cloud temporarily opens NSG rules for a specific source IP and duration. Combine for defense in depth.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Bastion Standard tier supports native client, file transfer, and scaling.</li>
              <li>Bastion Premium adds session recording and private-only deployment.</li>
              <li>JIT audits every access request; integrates with PIM for approval workflow.</li>
              <li>Bastion can be shared across peered VNets.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Azure Bastion</tag>
        <tag>JIT Access</tag>
        <tag>Secure Access</tag>
      </tags>
    </question>

    <question id="45" category-ref="cat-continuity" difficulty="advanced">
      <title>Data Consistency in Distributed Systems</title>
      <scenario>A globally distributed application needs data consistency. Some operations require strong consistency; others can tolerate eventual consistency for better performance.</scenario>
      <question-text>How should you design the data layer?</question-text>
      <choices>
        <choice letter="A">Eventual consistency for everything</choice>
        <choice letter="B">Strong consistency everywhere regardless of latency</choice>
        <choice letter="C">Cosmos DB with session consistency default and strong consistency for critical operations</choice>
        <choice letter="D">Local caching without database consistency</choice>
      </choices>
      <correct-answer>C</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Cosmos DB offers five consistency levels; choose per-operation to balance consistency and performance.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure Cosmos DB provides five consistency levels: Strong, Bounded Staleness, Session, Consistent Prefix, and Eventual. Session consistency (default) guarantees read-your-writes and is suitable for most operations. Override to Strong for critical operations like financial transactions.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Strong consistency doubles RU cost and adds latency for global writes.</li>
              <li>Session consistency uses session tokens; works well for user-specific data.</li>
              <li>Bounded Staleness guarantees maximum lag (time or operations).</li>
              <li>Consistency can be relaxed per-request but not strengthened beyond default.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Consistency</tag>
        <tag>Cosmos DB</tag>
        <tag>Distributed Systems</tag>
      </tags>
    </question>

    <question id="46" category-ref="cat-infrastructure" difficulty="advanced">
      <title>Application Modernization</title>
      <scenario>A legacy monolithic application needs modernization. The team wants to incrementally refactor to microservices while maintaining the existing application.</scenario>
      <question-text>What pattern enables incremental modernization?</question-text>
      <choices>
        <choice letter="A">Lift and shift only</choice>
        <choice letter="B">Complete rewrite before any deployment</choice>
        <choice letter="C">Strangler Fig pattern with API gateway routing between legacy and new services</choice>
        <choice letter="D">Parallel operation of two completely separate systems</choice>
      </choices>
      <correct-answer>C</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Strangler Fig incrementally replaces monolith functionality; API gateway routes traffic.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>The Strangler Fig pattern incrementally replaces monolith components with microservices. An API gateway (Azure API Management) routes requests: new functionality to microservices, remaining to monolith. Over time, the monolith shrinks until fully replaced.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Start with features that are well-defined, high-value, or need scaling.</li>
              <li>Anti-corruption layer translates between old and new data models.</li>
              <li>Event-driven integration decouples services during transition.</li>
              <li>Use feature flags to control traffic routing during migration.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Strangler Fig</tag>
        <tag>Modernization</tag>
        <tag>Microservices</tag>
      </tags>
    </question>

    <question id="47" category-ref="cat-governance" difficulty="advanced">
      <title>Cost Management Architecture</title>
      <scenario>An organization needs to allocate cloud costs to business units, set budgets with alerts, and identify optimization opportunities across 100 subscriptions.</scenario>
      <question-text>What cost management architecture should you implement?</question-text>
      <choices>
        <choice letter="A">Mandatory tags for cost allocation, Cost Management budgets and alerts, and Azure Advisor</choice>
        <choice letter="B">Single subscription with manual cost tracking</choice>
        <choice letter="C">No tags with estimation-based allocation</choice>
        <choice letter="D">Third-party cost tool without Azure integration</choice>
      </choices>
      <correct-answer>A</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Tags enable cost allocation; budgets prevent overruns; Advisor identifies savings opportunities.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Enforce mandatory tags (CostCenter, Department, Project) via Azure Policy. Cost Management groups and filters costs by tags for showback/chargeback. Set budgets at subscription or resource group level with alerts. Azure Advisor identifies right-sizing and reserved instance opportunities.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Cost Management + Billing: analyze spending, create budgets, export data.</li>
              <li>Budget alerts can trigger Action Groups for automated responses.</li>
              <li>Reserved Instances and Savings Plans reduce costs for predictable workloads.</li>
              <li>FinOps practices combine people, process, and technology for cost optimization.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Cost Management</tag>
        <tag>FinOps</tag>
        <tag>Tags</tag>
      </tags>
    </question>

    <question id="48" category-ref="cat-infrastructure" difficulty="advanced">
      <title>IoT Solution Architecture</title>
      <scenario>A manufacturing company needs to collect telemetry from 100,000 factory devices, process data for anomaly detection, store data for analytics, and send commands back to devices.</scenario>
      <question-text>What is the core Azure service for this IoT architecture?</question-text>
      <choices>
        <choice letter="A">Azure IoT Hub with device provisioning, twins, and direct methods</choice>
        <choice letter="B">Event Hubs only</choice>
        <choice letter="C">Direct HTTP calls to each device</choice>
        <choice letter="D">Azure Service Bus for all device communication</choice>
      </choices>
      <correct-answer>A</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>IoT Hub provides bidirectional device communication, provisioning, and device management.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure IoT Hub provides device-to-cloud telemetry ingestion, cloud-to-device commands (direct methods), device twins for state management, and Device Provisioning Service for zero-touch enrollment. Route telemetry to Stream Analytics for anomaly detection and storage for analytics.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Device twins: JSON documents representing device state and desired configuration.</li>
              <li>Direct methods: synchronous request-response calls to devices.</li>
              <li>IoT Edge runs intelligence at the edge with modules deployed from IoT Hub.</li>
              <li>Device Provisioning Service enables zero-touch, secure provisioning at scale.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>IoT Hub</tag>
        <tag>Device Management</tag>
        <tag>Manufacturing</tag>
      </tags>
    </question>

    <question id="49" category-ref="cat-data" difficulty="advanced">
      <title>Machine Learning Operations</title>
      <scenario>A data science team needs to train, deploy, and monitor ML models at scale with versioning, reproducibility, and automated retraining.</scenario>
      <question-text>Which platform provides end-to-end MLOps capabilities?</question-text>
      <choices>
        <choice letter="A">Custom Python scripts on VMs</choice>
        <choice letter="B">Azure Machine Learning with pipelines, model registry, and managed endpoints</choice>
        <choice letter="C">Jupyter notebooks without version control</choice>
        <choice letter="D">Azure Databricks only without model management</choice>
      </choices>
      <correct-answer>B</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Azure ML provides complete MLOps: experiment tracking, pipelines, model registry, deployment, and monitoring.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>Azure Machine Learning provides experiment tracking, reproducible pipelines, model versioning in the registry, managed online/batch endpoints for deployment, and monitoring for drift detection. Integrate with Azure DevOps or GitHub for CI/CD of ML workflows.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Pipelines: reusable, schedulable workflows for data prep, training, evaluation.</li>
              <li>Environments ensure reproducibility with Docker containers.</li>
              <li>Managed endpoints handle scaling, blue-green deployment, and traffic splitting.</li>
              <li>Data drift monitoring triggers retraining when model accuracy degrades.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Azure ML</tag>
        <tag>MLOps</tag>
        <tag>Model Management</tag>
      </tags>
    </question>

    <question id="50" category-ref="cat-infrastructure" difficulty="advanced">
      <title>Well-Architected Review</title>
      <scenario>Before going to production, you need to validate that your architecture follows Azure best practices across reliability, security, cost, operations, and performance.</scenario>
      <question-text>What framework and tools should guide your review?</question-text>
      <choices>
        <choice letter="A">Custom checklist without industry standards</choice>
        <choice letter="B">Azure Well-Architected Framework with Well-Architected Review assessment</choice>
        <choice letter="C">Only focus on cost optimization</choice>
        <choice letter="D">Skip review and deploy directly</choice>
      </choices>
      <correct-answer>B</correct-answer>
      <hints>
        <hint level="1" label="Brief Hint">
          <content>Well-Architected Framework provides pillar-based guidance; assessments generate actionable recommendations.</content>
        </hint>
        <hint level="2" label="Complete Explanation">
          <content>The Azure Well-Architected Framework provides best practices across five pillars: Reliability, Security, Cost Optimization, Operational Excellence, and Performance Efficiency. The Well-Architected Review assessment tool generates recommendations based on your workload's characteristics.</content>
        </hint>
        <hint level="3" label="Deep Knowledge">
          <content>
            <ul>
              <li>Workload-specific assessments available (AKS, SAP, Azure VMware Solution).</li>
              <li>Azure Advisor provides automated Well-Architected recommendations.</li>
              <li>Mission-critical workloads have additional design methodology.</li>
              <li>Conduct reviews before production and periodically thereafter.</li>
            </ul>
          </content>
        </hint>
      </hints>
      <tags>
        <tag>Well-Architected</tag>
        <tag>Best Practices</tag>
        <tag>Architecture Review</tag>
      </tags>
    </question>
  </questions>

  <glossary>
    <term id="gl-landing-zone" category="Architecture">
      <name>Landing Zone</name>
      <definition>A pre-configured Azure environment with networking, identity, governance, and security foundations for workload deployment.</definition>
      <exam-note>Azure Landing Zone accelerators provide reference implementations for enterprise-scale adoption.</exam-note>
    </term>
    <term id="gl-hub-spoke" category="Networking">
      <name>Hub-Spoke Topology</name>
      <definition>Network architecture with centralized shared services in a hub VNet connected to isolated workload spoke VNets via peering.</definition>
      <exam-note>Use UDRs to force spoke-to-spoke traffic through the hub for inspection.</exam-note>
    </term>
    <term id="gl-pim" category="Identity">
      <name>Privileged Identity Management</name>
      <definition>Azure AD feature providing just-in-time privileged access with approval workflows, time-limited activation, and audit trails.</definition>
      <exam-note>Reduces standing access risk; requires Entra ID P2 license.</exam-note>
    </term>
    <term id="gl-private-endpoint" category="Networking">
      <name>Private Endpoint</name>
      <definition>Network interface with private IP that connects to Azure PaaS services, keeping traffic on the Azure backbone.</definition>
      <exam-note>Requires Private DNS zones for name resolution to private IPs.</exam-note>
    </term>
    <term id="gl-cmk" category="Security">
      <name>Customer-Managed Keys</name>
      <definition>Encryption keys stored in customer's Key Vault, providing control over encryption key lifecycle for Azure services.</definition>
      <exam-note>Enables key rotation, revocation, and compliance with key management requirements.</exam-note>
    </term>
    <term id="gl-synapse-link" category="Data">
      <name>Azure Synapse Link</name>
      <definition>Near-real-time replication from operational databases (Cosmos DB, SQL) to Synapse Analytics for hybrid transactional/analytical processing.</definition>
      <exam-note>Eliminates ETL for analytics on operational data; no impact on transactional workload.</exam-note>
    </term>
    <term id="gl-strangler-fig" category="Architecture">
      <name>Strangler Fig Pattern</name>
      <definition>Application modernization pattern that incrementally replaces monolith components with microservices using an API gateway for routing.</definition>
      <exam-note>Enables gradual modernization without big-bang rewrites.</exam-note>
    </term>
    <term id="gl-composite-sla" category="Reliability">
      <name>Composite SLA</name>
      <definition>Overall availability calculated by multiplying SLAs of dependent services in series; parallel redundancy improves composite SLA.</definition>
      <exam-note>More dependent components = lower composite SLA; design redundancy accordingly.</exam-note>
    </term>
  </glossary>
</certification-exam>